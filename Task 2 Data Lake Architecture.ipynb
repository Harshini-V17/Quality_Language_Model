{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4894e589-587a-4607-a2c0-11945544250e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base paths created (or existed): dbfs:/FileStore/task2_datalake\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Paths\n",
    "lake_base = \"dbfs:/FileStore/task2_datalake\"\n",
    "raw_path    = f\"{lake_base}/raw\"\n",
    "bronze_path = f\"{lake_base}/bronze\"\n",
    "silver_path = f\"{lake_base}/silver\"\n",
    "gold_path   = f\"{lake_base}/gold\"\n",
    "\n",
    "# Ensure directories exist\n",
    "dbutils.fs.mkdirs(raw_path)\n",
    "dbutils.fs.mkdirs(bronze_path)\n",
    "dbutils.fs.mkdirs(silver_path)\n",
    "dbutils.fs.mkdirs(gold_path)\n",
    "\n",
    "print(\"Base paths created (or existed):\", lake_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eedb8ed5-5c94-4ee3-bf09-3cdd942183d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/task2_datalake/raw/source1_clinical_notes.jsonl</td><td>source1_clinical_notes.jsonl</td><td>81577</td><td>1763108073000</td></tr><tr><td>dbfs:/FileStore/task2_datalake/raw/source2_lab_reports.csv</td><td>source2_lab_reports.csv</td><td>14650</td><td>1763108073000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/task2_datalake/raw/source1_clinical_notes.jsonl",
         "source1_clinical_notes.jsonl",
         81577,
         1763108073000
        ],
        [
         "dbfs:/FileStore/task2_datalake/raw/source2_lab_reports.csv",
         "source2_lab_reports.csv",
         14650,
         1763108073000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "src_base = \"dbfs:/FileStore/task1_input_files\"\n",
    "\n",
    "# Copy raw source files into raw layer (keeps an immutable raw copy)\n",
    "dbutils.fs.cp(f\"{src_base}/source1_clinical_notes.jsonl\", f\"{raw_path}/source1_clinical_notes.jsonl\")\n",
    "dbutils.fs.cp(f\"{src_base}/source2_lab_reports.csv\", f\"{raw_path}/source2_lab_reports.csv\")\n",
    "\n",
    "display(dbutils.fs.ls(raw_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d267f0ac-5fdd-4847-b7ef-6d0bb01bed16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Schemas\n",
    "notes_schema = StructType([\n",
    "    StructField(\"patient_id\", StringType(), True),\n",
    "    StructField(\"patient_name\", StringType(), True),\n",
    "    StructField(\"note_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"clinical_text\", StringType(), True),\n",
    "    StructField(\"doctor_name\", StringType(), True),\n",
    "    StructField(\"hospital\", StringType(), True),\n",
    "    StructField(\"mrn\", StringType(), True)\n",
    "])\n",
    "\n",
    "lab_schema = StructType([\n",
    "    StructField(\"patient_id\", StringType(), True),\n",
    "    StructField(\"report_id\", StringType(), True),\n",
    "    StructField(\"test_name\", StringType(), True),\n",
    "    StructField(\"result_value\", DoubleType(), True),\n",
    "    StructField(\"unit\", StringType(), True),\n",
    "    StructField(\"report_date\", StringType(), True),\n",
    "    StructField(\"lab_name\", StringType(), True),\n",
    "    StructField(\"technician\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c577d8b-7a55-4447-a6fc-b4187a975d53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze notes: 200\nBronze labs : 200\n"
     ]
    }
   ],
   "source": [
    "# Ingest into BRONZE\n",
    "# Read raw files with schema, do minimal fixes and write to Delta in bronze\n",
    "notes_bronze = spark.read.schema(notes_schema).json(f\"{raw_path}/source1_clinical_notes.jsonl\") \\\n",
    "    .withColumn(\"timestamp\", F.to_timestamp(\"timestamp\")) \\\n",
    "    .withColumn(\"ingested_at\", F.current_timestamp()) \\\n",
    "    .withColumn(\"ingest_batch_id\", F.lit(str(uuid.uuid4())))\n",
    "\n",
    "labs_bronze = spark.read.option(\"header\", True).schema(lab_schema).csv(f\"{raw_path}/source2_lab_reports.csv\") \\\n",
    "    .withColumn(\"report_date\", F.to_date(\"report_date\", \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"ingested_at\", F.current_timestamp()) \\\n",
    "    .withColumn(\"ingest_batch_id\", F.lit(str(uuid.uuid4())))\n",
    "\n",
    "# Write as Delta (bronze)\n",
    "notes_bronze.write.format(\"delta\").mode(\"overwrite\").save(f\"{bronze_path}/notes\")\n",
    "labs_bronze.write.format(\"delta\").mode(\"overwrite\").save(f\"{bronze_path}/labs\")\n",
    "\n",
    "print(\"Bronze notes:\", spark.read.format(\"delta\").load(f\"{bronze_path}/notes\").count())\n",
    "print(\"Bronze labs :\", spark.read.format(\"delta\").load(f\"{bronze_path}/labs\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b668030-6921-4d05-831b-b498a6a563b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>format</th><th>id</th><th>name</th><th>description</th><th>location</th><th>createdAt</th><th>lastModified</th><th>partitionColumns</th><th>clusteringColumns</th><th>numFiles</th><th>sizeInBytes</th><th>properties</th><th>minReaderVersion</th><th>minWriterVersion</th><th>tableFeatures</th><th>statistics</th></tr></thead><tbody><tr><td>delta</td><td>24aa6a8e-445a-4cd8-9eeb-6b91b766ec90</td><td>null</td><td>null</td><td>dbfs:/FileStore/task2_datalake/bronze/notes</td><td>2025-11-14T08:15:54.107Z</td><td>2025-11-14T08:16:01Z</td><td>List()</td><td>List()</td><td>1</td><td>18728</td><td>Map()</td><td>1</td><td>2</td><td>List(appendOnly, invariants)</td><td>Map()</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "delta",
         "24aa6a8e-445a-4cd8-9eeb-6b91b766ec90",
         null,
         null,
         "dbfs:/FileStore/task2_datalake/bronze/notes",
         "2025-11-14T08:15:54.107Z",
         "2025-11-14T08:16:01Z",
         [],
         [],
         1,
         18728,
         {},
         1,
         2,
         [
          "appendOnly",
          "invariants"
         ],
         {}
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "format",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "createdAt",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "lastModified",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "partitionColumns",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "clusteringColumns",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "numFiles",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "sizeInBytes",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "properties",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "minReaderVersion",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "minWriterVersion",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "tableFeatures",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "statistics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"long\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>1</td><td>2025-11-14T08:20:46Z</td><td>1273012412979455</td><td>harshinivijayarajan@gmail.com</td><td>WRITE</td><td>Map(mode -> Append, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(3346931961398007)</td><td>1114-061524-4w2wzas5</td><td>0</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 4603)</td><td>null</td><td>Databricks-Runtime/15.4.x-scala2.12</td></tr><tr><td>0</td><td>2025-11-14T08:16:01Z</td><td>1273012412979455</td><td>harshinivijayarajan@gmail.com</td><td>WRITE</td><td>Map(mode -> Overwrite, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(3346931961398007)</td><td>1114-061524-4w2wzas5</td><td>null</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 1, numOutputRows -> 200, numOutputBytes -> 18728)</td><td>null</td><td>Databricks-Runtime/15.4.x-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "2025-11-14T08:20:46Z",
         "1273012412979455",
         "harshinivijayarajan@gmail.com",
         "WRITE",
         {
          "mode": "Append",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "3346931961398007"
         ],
         "1114-061524-4w2wzas5",
         0,
         "WriteSerializable",
         true,
         {
          "numFiles": "1",
          "numOutputBytes": "4603",
          "numOutputRows": "5"
         },
         null,
         "Databricks-Runtime/15.4.x-scala2.12"
        ],
        [
         0,
         "2025-11-14T08:16:01Z",
         "1273012412979455",
         "harshinivijayarajan@gmail.com",
         "WRITE",
         {
          "mode": "Overwrite",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "3346931961398007"
         ],
         "1114-061524-4w2wzas5",
         null,
         "WriteSerializable",
         false,
         {
          "numFiles": "1",
          "numOutputBytes": "18728",
          "numOutputRows": "200"
         },
         null,
         "Databricks-Runtime/15.4.x-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobRunId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in version 0: 200\n"
     ]
    }
   ],
   "source": [
    "# Delta versioning\n",
    "# Show current version info (Delta transaction log)\n",
    "display(spark.sql(f\"DESCRIBE DETAIL delta.`{bronze_path}/notes`\"))\n",
    "\n",
    "# Make a small append/change to create version 1 -> 2 (simulating new load)\n",
    "sample = notes_bronze.limit(5).withColumn(\"clinical_text\", F.concat(F.col(\"clinical_text\"), F.lit(\" [append test]\")))\n",
    "sample.write.format(\"delta\").mode(\"append\").save(f\"{bronze_path}/notes\")\n",
    "\n",
    "# Show versions after append\n",
    "display(spark.sql(f\"DESCRIBE HISTORY delta.`{bronze_path}/notes`\").limit(10))\n",
    "\n",
    "# Time travel: read previous version (version 0)\n",
    "df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(f\"{bronze_path}/notes\")\n",
    "print(\"Rows in version 0:\", df_v0.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b23b5d6-be01-4106-a72d-48ef205cb254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQ notes failed: 0\nDQ labs failed: 0\n"
     ]
    }
   ],
   "source": [
    "# Load bronze\n",
    "notes = spark.read.format(\"delta\").load(f\"{bronze_path}/notes\")\n",
    "labs = spark.read.format(\"delta\").load(f\"{bronze_path}/labs\")\n",
    "import json\n",
    "# Minimal redaction (use phi_rules.json from task1_input_files)\n",
    "phi = json.loads(dbutils.fs.head(\"dbfs:/FileStore/task1_input_files/phi_rules.json\", 10000))\n",
    "patterns = phi[\"patterns\"]\n",
    "redaction_format = phi[\"redaction_format\"]\n",
    "\n",
    "def apply_redaction_expr(col_expr):\n",
    "    expr = col_expr\n",
    "    for p in patterns:\n",
    "        expr = F.regexp_replace(expr, p[\"pattern\"], redaction_format.replace(\"{type}\", p[\"type\"].upper()))\n",
    "    return expr\n",
    "\n",
    "notes_silver = (notes\n",
    "                .withColumn(\"clinical_text\", apply_redaction_expr(F.col(\"clinical_text\")))\n",
    "                .withColumn(\"patient_name\", apply_redaction_expr(F.col(\"patient_name\")))\n",
    "                .withColumn(\"doctor_name\", apply_redaction_expr(F.col(\"doctor_name\")))\n",
    "                .withColumn(\"mrn\", apply_redaction_expr(F.col(\"mrn\")))\n",
    "                .dropDuplicates([\"note_id\"])\n",
    "                .withColumn(\"cleaned_at\", F.current_timestamp())\n",
    "               )\n",
    "\n",
    "labs_silver = (labs\n",
    "               .withColumn(\"lab_name\", apply_redaction_expr(F.col(\"lab_name\")))\n",
    "               .dropDuplicates([\"report_id\"])\n",
    "               .withColumn(\"cleaned_at\", F.current_timestamp())\n",
    "              )\n",
    "\n",
    "# Data quality example checks: non-null patient_id and timestamp\n",
    "dq_notes_failed = notes_silver.filter(F.col(\"patient_id\").isNull() | F.col(\"timestamp\").isNull())\n",
    "dq_labs_failed = labs_silver.filter(F.col(\"patient_id\").isNull() | F.col(\"report_date\").isNull())\n",
    "\n",
    "# Save silver as Delta\n",
    "notes_silver.write.format(\"delta\").mode(\"overwrite\").save(f\"{silver_path}/notes\")\n",
    "labs_silver.write.format(\"delta\").mode(\"overwrite\").save(f\"{silver_path}/labs\")\n",
    "\n",
    "print(\"DQ notes failed:\", dq_notes_failed.count())\n",
    "print(\"DQ labs failed:\", dq_labs_failed.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35b7bbec-16c0-46cb-9536-b62210b14ee9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QLM ready rows: 400\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>patient_id</th><th>source_record_id</th><th>event_timestamp</th><th>clinical_text</th><th>doctor_name</th><th>source_system</th><th>record_type</th></tr></thead><tbody><tr><td>P001</td><td>CN00001</td><td>2024-09-22T00:00:00Z</td><td>[REDACTED_PATIENT_NAME]. Presented with chest pain for 5 days. Assessment: chest pain. Plan: Start Aspirin 75mg. Contact: [REDACTED_PHONE_NUMBER]. MRN: [REDACTED_MRN]. [REDACTED_ADDRESS].</td><td>[REDACTED_DOCTOR_NAME]</td><td>EHR_Notes</td><td>clinical_note</td></tr><tr><td>P002</td><td>CN00002</td><td>2024-11-03T00:00:00Z</td><td>[REDACTED_PATIENT_NAME]. Presented with chest pain for 4 days. Assessment: chest pain. Plan: Start Aspirin 75mg. Contact: [REDACTED_PHONE_NUMBER]. MRN: [REDACTED_MRN]. [REDACTED_ADDRESS].</td><td>[REDACTED_DOCTOR_NAME]</td><td>EHR_Notes</td><td>clinical_note</td></tr><tr><td>P003</td><td>CN00003</td><td>2024-01-19T00:00:00Z</td><td>[REDACTED_PATIENT_NAME]. Presented with abdominal pain for 6 days. Assessment: abdominal pain. Plan: Start Pantoprazole 40mg. Contact: [REDACTED_PHONE_NUMBER]. MRN: [REDACTED_MRN]. [REDACTED_ADDRESS].</td><td>[REDACTED_DOCTOR_NAME]</td><td>EHR_Notes</td><td>clinical_note</td></tr><tr><td>P004</td><td>CN00004</td><td>2024-01-06T00:00:00Z</td><td>[REDACTED_PATIENT_NAME]. Presented with abdominal pain for 7 days. Assessment: abdominal pain. Plan: Start Pantoprazole 40mg. Contact: [REDACTED_PHONE_NUMBER]. MRN: [REDACTED_MRN]. [REDACTED_ADDRESS].</td><td>[REDACTED_DOCTOR_NAME]</td><td>EHR_Notes</td><td>clinical_note</td></tr><tr><td>P005</td><td>CN00005</td><td>2024-11-28T00:00:00Z</td><td>[REDACTED_PATIENT_NAME]. Presented with abdominal pain for 2 days. Assessment: abdominal pain. Plan: Start Pantoprazole 40mg. Contact: [REDACTED_PHONE_NUMBER]. MRN: [REDACTED_MRN]. [REDACTED_ADDRESS].</td><td>[REDACTED_DOCTOR_NAME]</td><td>EHR_Notes</td><td>clinical_note</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "P001",
         "CN00001",
         "2024-09-22T00:00:00Z",
         "[REDACTED_PATIENT_NAME]. Presented with chest pain for 5 days. Assessment: chest pain. Plan: Start Aspirin 75mg. Contact: [REDACTED_PHONE_NUMBER]. MRN: [REDACTED_MRN]. [REDACTED_ADDRESS].",
         "[REDACTED_DOCTOR_NAME]",
         "EHR_Notes",
         "clinical_note"
        ],
        [
         "P002",
         "CN00002",
         "2024-11-03T00:00:00Z",
         "[REDACTED_PATIENT_NAME]. Presented with chest pain for 4 days. Assessment: chest pain. Plan: Start Aspirin 75mg. Contact: [REDACTED_PHONE_NUMBER]. MRN: [REDACTED_MRN]. [REDACTED_ADDRESS].",
         "[REDACTED_DOCTOR_NAME]",
         "EHR_Notes",
         "clinical_note"
        ],
        [
         "P003",
         "CN00003",
         "2024-01-19T00:00:00Z",
         "[REDACTED_PATIENT_NAME]. Presented with abdominal pain for 6 days. Assessment: abdominal pain. Plan: Start Pantoprazole 40mg. Contact: [REDACTED_PHONE_NUMBER]. MRN: [REDACTED_MRN]. [REDACTED_ADDRESS].",
         "[REDACTED_DOCTOR_NAME]",
         "EHR_Notes",
         "clinical_note"
        ],
        [
         "P004",
         "CN00004",
         "2024-01-06T00:00:00Z",
         "[REDACTED_PATIENT_NAME]. Presented with abdominal pain for 7 days. Assessment: abdominal pain. Plan: Start Pantoprazole 40mg. Contact: [REDACTED_PHONE_NUMBER]. MRN: [REDACTED_MRN]. [REDACTED_ADDRESS].",
         "[REDACTED_DOCTOR_NAME]",
         "EHR_Notes",
         "clinical_note"
        ],
        [
         "P005",
         "CN00005",
         "2024-11-28T00:00:00Z",
         "[REDACTED_PATIENT_NAME]. Presented with abdominal pain for 2 days. Assessment: abdominal pain. Plan: Start Pantoprazole 40mg. Contact: [REDACTED_PHONE_NUMBER]. MRN: [REDACTED_MRN]. [REDACTED_ADDRESS].",
         "[REDACTED_DOCTOR_NAME]",
         "EHR_Notes",
         "clinical_note"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "patient_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "source_record_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "event_timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "clinical_text",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "doctor_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "source_system",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "record_type",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Canonicalize into one QLM-ready table (same approach as Task 1)\n",
    "notes_q = spark.read.format(\"delta\").load(f\"{silver_path}/notes\").select(\n",
    "    \"patient_id\",\n",
    "    F.col(\"note_id\").alias(\"source_record_id\"),\n",
    "    F.col(\"timestamp\").alias(\"event_timestamp\"),\n",
    "    \"clinical_text\",\n",
    "    F.col(\"doctor_name\").alias(\"doctor_name\"),\n",
    ").withColumn(\"source_system\", F.lit(\"EHR_Notes\")).withColumn(\"record_type\", F.lit(\"clinical_note\"))\n",
    "\n",
    "labs_q = spark.read.format(\"delta\").load(f\"{silver_path}/labs\").select(\n",
    "    \"patient_id\",\n",
    "    F.col(\"report_id\").alias(\"source_record_id\"),\n",
    "    F.col(\"report_date\").alias(\"event_timestamp\"),\n",
    "    F.concat_ws(\" | \", F.col(\"test_name\"), F.col(\"result_value\").cast(\"string\"), F.col(\"unit\")).alias(\"clinical_text\"),\n",
    ").withColumn(\"source_system\", F.lit(\"Labs\")).withColumn(\"record_type\", F.lit(\"lab_report\"))\n",
    "\n",
    "qlm_ready = notes_q.unionByName(labs_q, allowMissingColumns=True)\n",
    "\n",
    "qlm_ready.write.format(\"delta\").mode(\"overwrite\").save(f\"{gold_path}/qlm_ready\")\n",
    "\n",
    "# show a count and sample\n",
    "print(\"QLM ready rows:\", spark.read.format(\"delta\").load(f\"{gold_path}/qlm_ready\").count())\n",
    "spark.read.format(\"delta\").load(f\"{gold_path}/qlm_ready\").limit(5).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f70243a7-7809-4a36-afee-9beaa6125eb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 486 bytes.\nProvenance saved: dbfs:/FileStore/task2_datalake/provenance/provenance_c9e209ee-fcf8-424b-85dc-6b4299effbca.json\n"
     ]
    }
   ],
   "source": [
    "# Create a provenance record for the gold write\n",
    "provenance_path = f\"{lake_base}/provenance\"\n",
    "dbutils.fs.mkdirs(provenance_path)\n",
    "batch_id = str(uuid.uuid4())\n",
    "prov = {\n",
    "    \"batch_id\": batch_id,\n",
    "    \"created_at\": datetime.utcnow().isoformat(),\n",
    "    \"sources\": [\"source1_clinical_notes.jsonl\",\"source2_lab_reports.csv\"],\n",
    "    \"raw_path\": raw_path,\n",
    "    \"bronze_path\": bronze_path,\n",
    "    \"silver_path\": silver_path,\n",
    "    \"gold_path\": f\"{gold_path}/qlm_ready\",\n",
    "    \"dq_issues\": {\n",
    "        \"notes_failed\": int(dq_notes_failed.count()),\n",
    "        \"labs_failed\": int(dq_labs_failed.count())\n",
    "    }\n",
    "}\n",
    "dbutils.fs.put(f\"{provenance_path}/provenance_{batch_id}.json\", json.dumps(prov, indent=2), True)\n",
    "print(\"Provenance saved:\", f\"{provenance_path}/provenance_{batch_id}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f577da7-aaa8-4254-bcfb-db51932be32f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 757 bytes.\nAudit event created: dbfs:/FileStore/task2_datalake/audit/audit_ed8827c8-a2cc-4a06-a767-d3ae1b2fcba0.json\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/FileStore/task2_datalake/audit/audit_ed8827c8-a2cc-4a06-a767-d3ae1b2fcba0.json</td><td>audit_ed8827c8-a2cc-4a06-a767-d3ae1b2fcba0.json</td><td>757</td><td>1763108821000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/FileStore/task2_datalake/audit/audit_ed8827c8-a2cc-4a06-a767-d3ae1b2fcba0.json",
         "audit_ed8827c8-a2cc-4a06-a767-d3ae1b2fcba0.json",
         757,
         1763108821000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Databricks standard audit logs are enabled by workspace admin and land in cloud storage.\n",
    "# simulate an audit event by writing a small audit log for operations.\n",
    "\n",
    "audit_event = {\n",
    "  \"event_id\": str(uuid.uuid4()),\n",
    "  \"event_time\": datetime.utcnow().isoformat(),\n",
    "  \"user\": \"notebook_user\",\n",
    "  \"action\": \"write_gold_qlm_ready\",\n",
    "  \"target\": f\"{gold_path}/qlm_ready\",\n",
    "  \"details\": prov\n",
    "}\n",
    "\n",
    "dbutils.fs.put(f\"{lake_base}/audit/audit_{audit_event['event_id']}.json\", json.dumps(audit_event, indent=2), True)\n",
    "print(\"Audit event created:\", f\"{lake_base}/audit/audit_{audit_event['event_id']}.json\")\n",
    "\n",
    "# Show audit folder\n",
    "display(dbutils.fs.ls(f\"{lake_base}/audit\"))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Task 2 Data Lake Architecture",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}