{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21346ff1-8bba-4f86-bf44-a9bda9a53cf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Databricks notebook cell 1\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "dbutils.fs.mkdirs(\"dbfs:/FileStore/task1_input_files/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b92a1f12-af43-4fbe-a525-1e6faf0b007b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_base = \"dbfs:/FileStore/task1_input_files\"\n",
    "lake_base = \"dbfs:/FileStore/fda_task1_lake\"\n",
    "\n",
    "raw_path = f\"{lake_base}/raw\"\n",
    "curated_path = f\"{lake_base}/curated\"\n",
    "provenance_path = f\"{lake_base}/provenance\"\n",
    "\n",
    "dbutils.fs.mkdirs(raw_path)\n",
    "dbutils.fs.mkdirs(curated_path)\n",
    "dbutils.fs.mkdirs(provenance_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4480f2cf-dbf1-4d07-8b6f-40ba252f53fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "({'sources': [{'source_name': 'EHR_Clinical_Notes',\n",
       "    'file': 'source1_clinical_notes.jsonl',\n",
       "    'format': 'jsonl',\n",
       "    'description': 'Synthetic clinical notes with PHI-like fields'},\n",
       "   {'source_name': 'Lab_Reports',\n",
       "    'file': 'source2_lab_reports.csv',\n",
       "    'format': 'csv',\n",
       "    'description': 'Synthetic lab results CSV'}]},\n",
       " 'v1.0',\n",
       " ['source1_schema', 'source2_schema'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Metadata, PHI Rules, Schemas\n",
    "def load_json(path):\n",
    "    return json.loads(dbutils.fs.head(path, 10000))\n",
    "\n",
    "metadata = load_json(f\"{raw_base}/metadata.json\")\n",
    "phi_rules = load_json(f\"{raw_base}/phi_rules.json\")\n",
    "schema_defs = load_json(f\"{raw_base}/schema_definitions.json\")\n",
    "\n",
    "metadata, phi_rules[\"version\"], list(schema_defs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e332f780-0963-49ad-bea4-3588be46f9ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Define Spark Schemas\n",
    "source1_schema = StructType([\n",
    "    StructField(\"patient_id\", StringType(), True),\n",
    "    StructField(\"patient_name\", StringType(), True),\n",
    "    StructField(\"note_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"clinical_text\", StringType(), True),\n",
    "    StructField(\"doctor_name\", StringType(), True),\n",
    "    StructField(\"hospital\", StringType(), True),\n",
    "    StructField(\"mrn\", StringType(), True)\n",
    "])\n",
    "\n",
    "source2_schema = StructType([\n",
    "    StructField(\"patient_id\", StringType(), True),\n",
    "    StructField(\"report_id\", StringType(), True),\n",
    "    StructField(\"test_name\", StringType(), True),\n",
    "    StructField(\"result_value\", DoubleType(), True),\n",
    "    StructField(\"unit\", StringType(), True),\n",
    "    StructField(\"report_date\", StringType(), True),\n",
    "    StructField(\"lab_name\", StringType(), True),\n",
    "    StructField(\"technician\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242537fe-177b-4f4a-bcb9-f8a3fe0e8d1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ingest JSONL and CSV\n",
    "notes_df = spark.read.schema(source1_schema).json(f\"{raw_base}/source1_clinical_notes.jsonl\")\n",
    "labs_df = spark.read.option(\"header\",True).schema(source2_schema).csv(f\"{raw_base}/source2_lab_reports.csv\")\n",
    "\n",
    "notes_df = notes_df.withColumn(\"timestamp_ts\", F.to_timestamp(\"timestamp\"))\n",
    "labs_df = labs_df.withColumn(\"report_date_dt\", F.to_date(\"report_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15368861-3f0e-4a9a-b3f2-ef0f8cf94f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply PHI Redaction\n",
    "patterns = phi_rules[\"patterns\"]\n",
    "redaction_format = phi_rules[\"redaction_format\"]\n",
    "\n",
    "def redact(col):\n",
    "    expr = col\n",
    "    for r in patterns:\n",
    "        expr = F.regexp_replace(expr, r[\"pattern\"], redaction_format.replace(\"{type}\", r[\"type\"].upper()))\n",
    "    return expr\n",
    "\n",
    "red_notes = notes_df.select(\n",
    "    \"patient_id\",\"note_id\",\"timestamp_ts\",\n",
    "    redact(F.col(\"clinical_text\")).alias(\"clinical_text\"),\n",
    "    redact(F.col(\"doctor_name\")).alias(\"doctor_name\"),\n",
    "    redact(F.col(\"patient_name\")).alias(\"patient_name\"),\n",
    "    redact(F.col(\"hospital\")).alias(\"hospital\"),\n",
    "    redact(F.col(\"mrn\")).alias(\"mrn\")\n",
    ").withColumn(\"phi_rules_applied\", F.array([F.lit(r[\"type\"]) for r in patterns]))\n",
    "\n",
    "red_labs = labs_df.select(\n",
    "    \"patient_id\",\"report_id\",\"report_date_dt\",\n",
    "    F.concat_ws(\" | \", \"test_name\", F.col(\"result_value\").cast(\"string\"), \"unit\").alias(\"text\"),\n",
    "    redact(F.col(\"lab_name\")).alias(\"hospital\"),\n",
    "    F.lit(None).alias(\"doctor_name\"),\n",
    "    F.array([F.lit(r[\"type\"]) for r in patterns]).alias(\"phi_rules_applied\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e795537-702d-4603-9a45-ce78b1685192",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build QLM-Ready Dataset\n",
    "qlm_notes = red_notes.select(\n",
    "    \"patient_id\",\n",
    "    F.col(\"note_id\").alias(\"source_record_id\"),\n",
    "    F.col(\"timestamp_ts\").alias(\"event_timestamp\"),\n",
    "    \"clinical_text\",\n",
    "    \"doctor_name\",\n",
    "    \"hospital\",\n",
    "    \"phi_rules_applied\"\n",
    ").withColumn(\"source_system\", F.lit(\"EHR_Notes\")).withColumn(\"record_type\", F.lit(\"clinical_note\"))\n",
    "\n",
    "qlm_labs = red_labs.select(\n",
    "    \"patient_id\",\n",
    "    F.col(\"report_id\").alias(\"source_record_id\"),\n",
    "    \"report_date_dt\",\n",
    "    F.col(\"text\").alias(\"text\"),\n",
    "    \"doctor_name\",\n",
    "    \"hospital\",\n",
    "    \"phi_rules_applied\"\n",
    ").withColumn(\"source_system\", F.lit(\"Labs\")).withColumn(\"record_type\", F.lit(\"lab_report\"))\n",
    "\n",
    "qlm_df = qlm_notes.unionByName(qlm_labs, allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a157720a-b8a9-4374-9846-1c3caa6d6ef0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Row Hash and Batch Hash\n",
    "batch_id = str(uuid.uuid4())\n",
    "ingested_at = datetime.utcnow().isoformat()\n",
    "\n",
    "fields_to_hash = [\"patient_id\",\"source_record_id\",\"event_timestamp\",\"text\",\"source_system\",\"record_type\"]\n",
    "\n",
    "qlm_hashed = qlm_df.withColumn(\n",
    "    \"row_hash\",\n",
    "    F.sha2(F.concat_ws(\"||\", *[F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in fields_to_hash]), 256)\n",
    ").withColumn(\"batch_id\", F.lit(batch_id))\n",
    "\n",
    "batch_info = qlm_hashed.agg(\n",
    "    F.sha2(F.concat_ws(\"\", F.sort_array(F.collect_list(\"row_hash\"))), 256).alias(\"batch_sha256\"),\n",
    "    F.count(\"*\").alias(\"row_count\")\n",
    ").collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89133eed-423d-4df5-93f8-502b2bde3380",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Save QLM Data & Provenance\n",
    "qlm_path = f\"{curated_path}/qlm_ready/batch_id={batch_id}\"\n",
    "prov_path = f\"{provenance_path}/run_id={batch_id}\"\n",
    "\n",
    "qlm_hashed.write.mode(\"overwrite\").parquet(qlm_path)\n",
    "\n",
    "prov_data = [{\n",
    "    \"batch_id\": batch_id,\n",
    "    \"ingested_at\": ingested_at,\n",
    "    \"sources\": [\"EHR_Clinical_Notes\",\"Lab_Reports\"],\n",
    "    \"source_files\": [\"source1_clinical_notes.jsonl\",\"source2_lab_reports.csv\"],\n",
    "    \"phi_rules_applied\": [r[\"type\"] for r in patterns],\n",
    "    \"row_count\": batch_info[\"row_count\"],\n",
    "    \"batch_sha256\": batch_info[\"batch_sha256\"],\n",
    "    \"qlm_output_path\": qlm_path\n",
    "}]\n",
    "\n",
    "prov_df = spark.createDataFrame(prov_data)\n",
    "prov_df.write.mode(\"append\").parquet(prov_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c3ea8a6-9ca3-4e04-a6dd-4984c0381b3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored : 3556e64d22654aa66bbfb41aeaada2f92048d0583883e6e5b740f8611d49d7ed\nRecomputed: 3556e64d22654aa66bbfb41aeaada2f92048d0583883e6e5b740f8611d49d7ed\n✔ Integrity Verified\n"
     ]
    }
   ],
   "source": [
    "# Verify Integrity\n",
    "loaded = spark.read.parquet(qlm_path)\n",
    "\n",
    "recomputed = loaded.agg(\n",
    "    F.sha2(F.concat_ws(\"\", F.sort_array(F.collect_list(\"row_hash\"))), 256)\n",
    ").collect()[0][0]\n",
    "\n",
    "print(\"Stored :\", batch_info[\"batch_sha256\"])\n",
    "print(\"Recomputed:\", recomputed)\n",
    "\n",
    "assert recomputed == batch_info[\"batch_sha256\"], \"❌ Integrity check failed!\"\n",
    "print(\"✔ Integrity Verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db11882-7c69-4bdd-a1ae-d60a5ee9f0d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n  \"batch_id\": \"0df5c6cc-7bc7-4318-bd0f-fb79dab20e11\",\n  \"batch_sha256\": \"3556e64d22654aa66bbfb41aeaada2f92048d0583883e6e5b740f8611d49d7ed\",\n  \"ingested_at\": \"2025-11-14T06:45:20.987530\",\n  \"phi_rules_applied\": [\n    \"doctor_name\",\n    \"patient_name\",\n    \"phone_number\",\n    \"mrn\",\n    \"address\"\n  ],\n  \"qlm_output_path\": \"dbfs:/FileStore/fda_task1_lake/curated/qlm_ready/batch_id=0df5c6cc-7bc7-4318-bd0f-fb79dab20e11\",\n  \"row_count\": 400,\n  \"source_files\": [\n    \"source1_clinical_notes.jsonl\",\n    \"source2_lab_reports.csv\"\n  ],\n  \"sources\": [\n    \"EHR_Clinical_Notes\",\n    \"Lab_Reports\"\n  ]\n}\nWrote 597 bytes.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get provenance as Python dictionary\n",
    "sample = prov_df.toPandas().to_dict(orient=\"records\")[0]\n",
    "\n",
    "# Convert any numpy arrays to python lists (json serializable)\n",
    "for key, value in sample.items():\n",
    "    if hasattr(value, \"tolist\"):\n",
    "        sample[key] = value.tolist()\n",
    "\n",
    "import json\n",
    "pretty = json.dumps(sample, indent=2)\n",
    "\n",
    "print(pretty)\n",
    "\n",
    "# Save JSON to DBFS\n",
    "dbutils.fs.put(f\"{prov_path}/provenance_sample.json\", pretty, True)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Task 1 FDA",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}